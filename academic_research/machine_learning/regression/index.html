<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#9400D3"><meta name="author" content="Joaquin Chou"><meta name="copyright" content="Joaquin Chou"><meta name="generator" content="Hexo 6.3.0"><meta name="theme" content="hexo-theme-yun"><title>Regression | 喵语小站</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...true?.options,
  });
});</script><link rel="icon" type="image/png" href="/favicon.ico"><link rel="mask-icon" href="/favicon.ico" color="#9400D3"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"www.joaquinchou.com","root":"/","title":"喵星暖暖窝","version":"1.10.11","mode":"time","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"algolia":{"appID":"L0D85KPS1S","apiKey":"715edff2e91248577c19b580f3efcb3a","indexName":"prod_NAME","hits":{"per_page":8}},"fireworks":{"colors":["0,250,154","255,69,0","33, 78, 194"]},"waline":{"config":{"enable":true,"serverURL":"https://blog-api-joaquinchou.vercel.app","comment":false,"el":"#waline","lang":"zh-CN"},"cdn":"https://fastly.jsdelivr.net/npm/@waline/client@v2/dist/waline.js","dark":"html.dark"},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><script>(function(){
  var bp = document.createElement('script');
  var curProtocol = window.location.protocol.split(':')[0];
  if (curProtocol === 'https') {
    bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
  }
  else {
    bp.src = 'http://push.zhanzhang.baidu.com/push.js';
  }
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(bp, s);
})();</script><meta name="description" content="&amp;emsp;&amp;emsp;这是我发的第一篇博客，抱着好玩的心态基于hexo搭建属于自己的网站，这里要非常感谢云游大大的云主题。关于机器学习的学习，今年5月份研究生上岸后开始接触，期间跟着B站台大李宏毅老师的课学习倒也写了四篇Markdown，后来到后面深度学习可写的东西太多了，果断不写了，学习它的思想。但是新博客还是要放点东西试试水的，空荡荡岂不很难受哈哈哈，以后更新尽量以总结性的为主。加油搬砖～">
<meta property="og:type" content="article">
<meta property="og:title" content="Regression">
<meta property="og:url" content="https://www.joaquinchou.com/academic_research/machine_learning/regression/index.html">
<meta property="og:site_name" content="喵语小站">
<meta property="og:description" content="&amp;emsp;&amp;emsp;这是我发的第一篇博客，抱着好玩的心态基于hexo搭建属于自己的网站，这里要非常感谢云游大大的云主题。关于机器学习的学习，今年5月份研究生上岸后开始接触，期间跟着B站台大李宏毅老师的课学习倒也写了四篇Markdown，后来到后面深度学习可写的东西太多了，果断不写了，学习它的思想。但是新博客还是要放点东西试试水的，空荡荡岂不很难受哈哈哈，以后更新尽量以总结性的为主。加油搬砖～">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s3.ax1x.com/2020/11/15/DFjqpD.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/05/asXN1H.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/05/asXGtO.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/05/asXJhD.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/05/asXUcd.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/05/asXt9e.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/05/asXJhD.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/05/asXBHP.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/05/asXajA.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/05/asXwnI.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/05/asXy4S.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/05/asXc9g.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/05/asXy4S.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/05/asXc9g.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/05/asXc9g.png">
<meta property="og:image" content="https://s3.ax1x.com/2020/11/15/DF9qG6.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/05/asXg3Q.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/05/asXRjs.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/05/asXoNT.png">
<meta property="og:image" content="https://s3.ax1x.com/2020/11/15/DFP3tI.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/05/asXfun.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/05/asX4H0.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/05/asXhBq.png">
<meta property="og:image" content="https://s3.ax1x.com/2020/11/15/DFi2Gt.png">
<meta property="article:published_time" content="2020-08-04T16:00:00.000Z">
<meta property="article:modified_time" content="2022-03-06T15:42:17.478Z">
<meta property="article:author" content="Joaquin Chou">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s3.ax1x.com/2020/11/15/DFjqpD.jpg"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><canvas id="trianglifyContainer"></canvas><script defer src="https://fastly.jsdelivr.net/npm/trianglify@4/dist/trianglify.bundle.js"></script><script>document.addEventListener("DOMContentLoaded", () => {
  const pattern = trianglify({
    width: 800,
    height: 600,
    cellSize: 75,
    palette: ["Oranges", "RdBu", "Purples", "Blues"],
  });
  const canvasOpts = {
    applyCssScaling: false
  }
  document.body.appendChild(pattern.toCanvas(trianglifyContainer, canvasOpts));
});</script><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="Joaquin Chou"><img width="96" loading="lazy" src="https://joaquinchou-1259120139.cos.ap-guangzhou.myqcloud.com/img/img_config/JTpLkT.jpg" alt="Joaquin Chou"></a><div class="site-author-name"><a href="/about/">Joaquin Chou</a></div><a class="site-name" href="/about/site.html">喵语小站</a><sub class="site-subtitle">Hope that someone you love would also love you!</sub><div class="site-description">希望你喜欢的人她/他也爱你！</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">17</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">4</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">8</span></a></div><a class="site-state-item hty-icon-button" href="/404.html" title="还没想好呢"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/JoaquinChou" title="GitHub" target="_blank" style="color:#FF00FF"><span class="icon iconify" data-icon="ri:github-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://music.163.com/#/my/m/music/playlist?id=621567138" title="网易云音乐" target="_blank" style="color:#C20C0C"><span class="icon iconify" data-icon="ri:netease-cloud-music-line"></span></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="我的小伙伴们" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a><a class="links-item hty-icon-button" href="/albums/" title="我的相册" style="color:#DA70D6"><span class="icon iconify" data-icon="ri:gallery-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.</span> <span class="toc-text">线性回归模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BB%BA%E6%A8%A1"><span class="toc-number">2.1.</span> <span class="toc-text">建模</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">2.2.</span> <span class="toc-text">计算损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%BE%E5%87%BA%E4%BD%BF%E5%BE%97%E6%8D%9F%E5%A4%B1%E6%9C%80%E5%B0%8F%E7%9A%84%E8%BE%93%E5%85%A5"><span class="toc-number">2.3.</span> <span class="toc-text">找出使得损失最小的输入</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E5%81%9A%E6%B3%95"><span class="toc-number">2.3.1.</span> <span class="toc-text">梯度下降的做法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E8%BF%87%E7%A8%8B%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">2.3.2.</span> <span class="toc-text">梯度下降的过程可视化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%8F%AF%E8%83%BD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">2.3.3.</span> <span class="toc-text">梯度下降可能遇到的问题</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%B1%82%E8%A7%A3"><span class="toc-number">2.4.</span> <span class="toc-text">梯度下降法求解</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%95%E7%89%B9%E5%BE%81%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B"><span class="toc-number">3.</span> <span class="toc-text">单特征回归模型预测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98"><span class="toc-number">4.</span> <span class="toc-text">过拟合问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E5%BF%B5"><span class="toc-number">4.1.</span> <span class="toc-text">概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E5%9B%BE%E5%BD%A2%E5%8C%96%E8%A7%A3%E9%87%8A"><span class="toc-number">4.2.</span> <span class="toc-text">过拟合图形化解释</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E8%A7%A3%E5%86%B3%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98"><span class="toc-number">4.3.</span> <span class="toc-text">正则化解决过拟合问题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E6%A6%82%E5%BF%B5"><span class="toc-number">4.3.1.</span> <span class="toc-text">正则化概念</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%83%BD%E8%A7%A3%E5%86%B3%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E5%8E%9F%E5%9B%A0"><span class="toc-number">4.3.2.</span> <span class="toc-text">能解决过拟合的原因</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E6%95%88%E6%9E%9C"><span class="toc-number">4.3.3.</span> <span class="toc-text">正则化的效果</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E7%89%B9%E5%BE%81%E7%9A%84%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B"><span class="toc-number">5.</span> <span class="toc-text">多特征的回归模型预测</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E5%85%A5%E5%AE%9D%E5%8F%AF%E6%A2%A6%E7%A7%8D%E7%B1%BB%E7%89%B9%E5%BE%81"><span class="toc-number">5.1.</span> <span class="toc-text">加入宝可梦种类特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E5%85%A5%E5%AE%9D%E5%8F%AF%E6%A2%A6%E6%9B%B4%E5%A4%9A%E7%89%B9%E5%BE%81"><span class="toc-number">5.2.</span> <span class="toc-text">加入宝可梦更多特征</span></a></li></ol></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#9400D3;"><link itemprop="mainEntityOfPage" href="https://www.joaquinchou.com/academic_research/machine_learning/regression/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="Joaquin Chou"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="喵语小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Regression</h1><div class="post-meta"><div class="post-time" style="display:block"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="创建时间：2020-08-05 00:00:00" itemprop="dateCreated datePublished" datetime="2020-08-05T00:00:00+08:00">2020-08-05</time></div><span class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><span class="icon iconify" data-icon="ri:file-word-line"></span></span> <span title="本文字数">3.7k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><span class="icon iconify" data-icon="ri:timer-line"></span></span> <span title="阅读时长">13m</span></span></span><span class="post-busuanzi"><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读次数"><span class="icon iconify" data-icon="ri:eye-line"></span> <span id="busuanzi_value_page_pv"></span></span></span><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/%E5%96%B5%E8%AF%AD%E7%AC%94%E8%AE%B0/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">喵语笔记</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="--text-color:#3776ab"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">机器学习</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><p>&emsp;&emsp;这是我发的第一篇博客，抱着好玩的心态基于hexo搭建属于自己的网站，这里要非常感谢<a target="_blank" rel="noopener" href="https://www.yunyoujun.cn/">云游大大</a>的云主题。关于机器学习的学习，今年5月份研究生上岸后开始接触，期间跟着B站台大<a target="_blank" rel="noopener" href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html">李宏毅</a>老师的课学习倒也写了四篇Markdown，后来到后面深度学习可写的东西太多了，果断不写了，学习它的思想。但是新博客还是要放点东西试试水的，空荡荡岂不很难受哈哈哈，以后更新尽量以总结性的为主。加油搬砖～<br><img src="https://s3.ax1x.com/2020/11/15/DFjqpD.jpg" alt="DFjqpD.jpg" border="0" / loading="lazy"></p>
<span id="more"></span>

<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a><strong>概述</strong></h2><p>&emsp;&emsp;在监督学习中，应用分为<b>回归问题，分类问题和标注问题</b>。回归问题一般是对连续值的处理，像预测房价，气温，销售额等。常用<font color='red'><b>线性回归</b></font>；分类问题模型一般输出是离散值，像图像分类，垃圾邮件识别，疾病检测等。常用<font color='red'><b>softmax回归</b></font>。所以回归应用场景十分广泛。<br><br><br></p>
<h2 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a><strong>线性回归模型</strong></h2><p>&emsp;&emsp;为什么我们要对宝可梦的CP值（Combat Point的简称,属于战斗点数，CP值越高，战斗力越强。）进行预测呢？做这件事的<strong>理由是</strong>如果预测的宝可梦CP值较大，我们可以用它来进化，再去打道馆；如果预测的CP值小，我们就不用它进化而是做成宝可梦糖果。预测CP值的目的是为了节约资源和培养成本。</p>
<p>&emsp;&emsp;了解完动机后，我们该怎么预测呢？我们进行预测的前提条件是已经拥有一定数量的宝可梦的特征（像HP，进化前的CP值，weight,height,etc.）和它进化后的CP值。然后想办法（这里指回归）根据输入是特征和输出是<b>CP值<font color='red'>找到一个函数$f$，使得$f$尽量接近生成宝可梦CP值的函数</font></b>。整个过程图形化如下图所示。</p>
<img src="https://s1.ax1x.com/2020/08/05/asXN1H.png" alt="asXN1H.png" border="0" style="zoom:67%;"/ loading="lazy">

<p>&emsp;&emsp;那怎么去找这样一个函数$f$呢？主要分为三步(第四步是计算)：</p>
<p>&emsp;&emsp;本次主要采用线性回归的方法，设$y$表示宝可梦进化后的CP值，$y^n$表示第$n$只宝可梦进化后的CP值，$x_i^n$表示第$n$只宝可梦的第$i$个特征。$w$表示权重（weight），$b$表示偏差（bias）。<br><br></p>
<h3 id="建模"><a href="#建模" class="headerlink" title="建模"></a><strong>建模</strong></h3><p>&emsp;&emsp;如果我们只考虑进化前的CP这一个特征，则可建模为$y=b+\omega x_{cp}$，我们的任务是从一系列<strong>不同的$w$和$b$组成的函数集合</strong>找到一个最接近于目标函数（宝可梦的进化是按照某个函数来生成的）的函数$f$；</p>
<p>&emsp;&emsp;但是在实际求解时，我们一般还会考虑更多的特征。此时建模为$y=b+\sum w_{i}x_{i}$，（$x_i:x_{cp},x_{hp},x_w,x_h$,……）我们的任务不变，还是是从一系列<strong>不同的$w$和$b$组成的函数集合</strong>中找到一个最接近于目标函数（宝可梦的进化是按照某个函数来生成的）的函数$f$；<br><br></p>
<h3 id="计算损失函数"><a href="#计算损失函数" class="headerlink" title="计算损失函数"></a><strong>计算损失函数</strong></h3><p>&emsp;&emsp;设$\hat y^n$表示第$n$只宝可梦进化后的<b><font color='red'>真实CP值</font></b>，$f(x_{cp}^n)$表示第$n$只宝可梦预测进化后的CP值，$x_i^n$表示第$n$只宝可梦的第$i$个特征。$w$表示权重（weight），$b$表示偏差（bias）。</p>
<p>&emsp;&emsp;为简单起见，以下谈论只考虑进化前的CP值这一个特征，则可建模为$y=b+wx_{cp}$，训练集是抓到的10只宝可梦，并且它们进化后的CP值已知。以进化前的CP值为横坐标，进化后的CP值为纵坐标。绘制成图为<br><img src="https://s1.ax1x.com/2020/08/05/asXGtO.png" alt="asXGtO.png" style="zoom:70%;" / loading="lazy"></p>
<p>&emsp;&emsp;得到训练集数据和模型后，我们定义$Loss Function$（损失函数）为$L(f)$,它的输入是一个函数$f$，输出是判断的这个函数的好坏，其中$L$的值越小代表$f$越好。在本实例中定义为<br>$$ L(f)=\sum_{n=1}^{10}((\hat y^n-f(x_{cp}^n)))^2 $$</p>
<p>$w$,$b$代入，也就是<br>$$ L(w,b)=\sum_{n=1}^{10}(\hat y^n-(b+wx_{cp}^n))^2 $$</p>
<p>我们也可以把$b-w$的坐标轴画出来，观察下$w,b$对$L$的影响。如下所示。<br><img src="https://s1.ax1x.com/2020/08/05/asXJhD.png" alt="asXJhD.png" border="0" style="zoom:67%;"/ loading="lazy"></p>
<p>由图可知，红色部分$Loss$的值较大，$w$和$b$越偏蓝代表$Loss$的值越小，即函数的拟合性能越好。<br><br></p>
<h3 id="找出使得损失最小的输入"><a href="#找出使得损失最小的输入" class="headerlink" title="找出使得损失最小的输入"></a><strong>找出使得损失最小的输入</strong></h3><p>&emsp;&emsp;损失函数值越小，说明函数的越接近目标函数。使损失函数最小的那个$w$和$b$就是最好的函数。即求解$f^*=\underset{x}{\operatorname {arg,min}}L(f)$<br>或为$$ w^*,b^*=\underset {w,b}{\operatorname {arg,min}}L(w,b)=\underset {w,b}{\operatorname {arg,min}}\sum_{n=1}^{10}(\hat y^n-(b+wx_{cp}^n))^2 $$<br>&emsp;&emsp;简单说就是求解能让$Loss$最小的$w$和$b$。如果学过线性回归的话，其实可以用<strong>最小二乘法</strong>进行求解出来。但是在李老师的课时，选用的是<b>梯度下降（Gradient Descent)</b>的方法来求解方程<b>梯度下降的好处</b>在于，<b><font color='red'>不论损失函数$L$再复杂，只要它是可微的，都可以采用梯度下降来找到比较好的结果。</font></b></p>
<h4 id="梯度下降的做法"><a href="#梯度下降的做法" class="headerlink" title="梯度下降的做法"></a><strong>梯度下降的做法</strong></h4><p>&emsp;&emsp;那梯度下降具体的原理和做法是怎么的呢？本节暂时介绍做法，原理放在下一节讲。假设我们现在只考虑求解一个参数$w$的情况。</p>
<p>此时方程为$w^*=\underset {w}{\operatorname {arg,min}}L(w)$，假设$L(w)$的函数图像如下棕色线所示。我们的任务就变成了在一个“山峰”中找到最低点。具体做法是<br>（1）随机选取一点作为初始化的$w^0$;</p>
<p>​（2）计算$\frac{dL}{dw}|_{w={w^0}}$;</p>
<p>（3）把$w^0-\eta\frac{dL}{dw}|_{w={w^0}}$的值赋给$w^1$进行更新。</p>
<p>​$\eta$是人为定义的学习率，对$w$的值进行更新的目标是往$L$最小值的方向逼近，所以当计算微分值（斜率）为正时，要减小$w$的值；为负时要</p>
<p>增加$w$的值。并且增加量取决于微分值$\frac{dL}{dw}$和学习率$\eta$，<b><font color='red'>相同微分值的情况下增大学习率，更新幅度大；减少学习率，更新幅度小。</font></b></p>
<p>​（4）以上过程只走了一步，实际上想要到达最低点要不断地进行以上两步进行迭代，最后可能走到局部最优值（Local Minima）或全局最优值(Gobal Minima)。但是结果确实可能陷入局部最小值，这取决于初始化的$w^0$的位置。<br><img src="https://s1.ax1x.com/2020/08/05/asXUcd.png" alt="asXUcd.png" border="0" style="zoom:67%;"/ loading="lazy"></p>
<p>&emsp;&emsp;那回到原来的问题，当我们有两个参数$w,b$时，怎么进行梯度下降呢？</p>
<p>同理，做法雷同一个参数的情形，只不过变成了原函数对两个参数分别求偏导。具体做法如下PPT所示，在此不在叙述。<br><img src="https://s1.ax1x.com/2020/08/05/asXt9e.png" alt="asXt9e.png" style="zoom:75%;"/ loading="lazy"></p>
<p>由两个参数的偏导数组成的矩阵就是函数$L$梯度。</p>
<h4 id="梯度下降的过程可视化"><a href="#梯度下降的过程可视化" class="headerlink" title="梯度下降的过程可视化"></a><strong>梯度下降的过程可视化</strong></h4><p>&emsp;&emsp;梯度下降寻找最小值的过程如下。<br><img src="https://s1.ax1x.com/2020/08/05/asXJhD.png" alt="asXJhD.png" border="0" style="zoom: 67%;"/ loading="lazy"></p>
<p>其中$\frac{\partial{L}}{\partial{b}}$和$\frac{\partial{L}}{\partial{\omega}}$是曲线的法线方向，通过学习率$\eta$控制下一步走的前后方向，可见通过梯度下降，能不断在连续函数的某个区域中找到一个最小值，也就是图中的蓝色区域。</p>
<h4 id="梯度下降可能遇到的问题"><a href="#梯度下降可能遇到的问题" class="headerlink" title="梯度下降可能遇到的问题"></a><strong>梯度下降可能遇到的问题</strong></h4><p>&emsp;&emsp;我们已经知道梯度下降方法是不管函数多复杂，只要它是可微的，总能求解出一个极值。那我们是否可以说经过多次更新参数后，我们都能得到最优的参数使得损失函数最小呢？不妨看下图的情况。<br><img src="https://s1.ax1x.com/2020/08/05/asXBHP.png" alt="asXBHP.png" border="0" style="zoom:67%;"/ loading="lazy"></p>
<p>&emsp;由图可知，<b><font color='red'>经过多次更新参数后，我们不一定能得到最优的参数使得损失函数最小，因为极值点或者驻点不一定函数的最值点。换言之，梯度下降方法容易受初始值$\omega^0$的位置影响而陷入局部最小值。</font></b></p>
<p>&emsp;&emsp;但幸运的是，在线性回归中，我们的<b>损失函数$L$没有局部最优解</b>。（因为无论是单变量还是多变量的线性回归，损失函数$L$都是凸函数。）<br><br></p>
<h3 id="梯度下降法求解"><a href="#梯度下降法求解" class="headerlink" title="梯度下降法求解"></a><strong>梯度下降法求解</strong></h3><p>&emsp;&emsp;在了解完梯度下降的方法后，利用这个方法来解方程$L(w,b)=\sum_{n=1}^{10}(\hat y^n-(b+wx_{cp}^n))^2$，计算步骤如下。<br><img src="https://s1.ax1x.com/2020/08/05/asXajA.png" alt="asXajA.png" border="0" style="zoom:67%;"/ loading="lazy"></p>
<p>（注：对$b$求偏导最后少了一个$-1$。）</p>
<p>计算结果为<br><img src="https://s1.ax1x.com/2020/08/05/asXwnI.png" alt="asXwnI.png" border="0" style="zoom: 67%;"/ loading="lazy"></p>
<p>&emsp;&emsp;求得的解为$b=-188.4,\omega=2.7$,除此之外，在整个求解过程中我们更关心在测试集上的$Average Error=35.0$,是否可以将其降得更低？<br><br><br></p>
<h2 id="单特征回归模型预测"><a href="#单特征回归模型预测" class="headerlink" title="单特征回归模型预测"></a><strong>单特征回归模型预测</strong></h2><p>&emsp;&emsp;为了让测试集上的$Average Error=35.0$更好地下降，我们可以换用更复杂的模型，即回到第一步建模。将模型进行修改。模型修改后的计算结果如下。（这里补充一点：是不是能画出直线就是线性模型，各种复杂的曲线就是非线性模型？ 其实还是线性模型，因为把 $x_{cp}^1 = (x_{cp})^2$ 看作一个特征，那么 $y = b + w_1·x_{cp} + w_2·x_{cp}^1$ 其实就是线性模型。）</p>
<div class="main">
    <img src="https://s1.ax1x.com/2020/08/05/asXy4S.png" alt="asXy4S.png" border="0" style="zoom:50%;"/ loading="lazy">
    <img src="https://s1.ax1x.com/2020/08/05/asXc9g.png" alt="asXc9g.png" border="0" style="zoom:50%;"/ loading="lazy">

</div>

<center>
<figure>
<img src="https://s1.ax1x.com/2020/08/05/asXy4S.png" alt="asXy4S.png" border="0" style="zoom:11%;"/ loading="lazy">
<img src="https://s1.ax1x.com/2020/08/05/asXc9g.png" alt="asXc9g.png" border="0" style="zoom:11%;"/ loading="lazy">

</figure>
</center>
&emsp;&emsp;由图可知，换用更复杂的模型后在训练集和测试集上的$Average Error$得到了更小的值，效果也更好。
那么模型是不是越复杂越好呢？这就引出了下面讨论的<b>过拟合（Overfitting）问题</b>。
<br><br>

<h2 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a><strong>过拟合问题</strong></h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a><strong>概念</strong></h3><p>&emsp;&emsp;由上述模型我们得到了一个更好的效果，所以继续增加更高次方的模型。结果如下图所示。</p>
<center>
<figure>
<img src="https://s1.ax1x.com/2020/08/05/asXc9g.png" alt="asXc9g.png" border="0" style="zoom: 50%;"/ loading="lazy">
<img src="https://s3.ax1x.com/2020/11/15/DF9qG6.png" alt="DF9qG6.png" border="0" style="zoom: 50%;"/ loading="lazy">
</figure>
</center>


<p>&emsp;&emsp;什么是过拟合问题呢？由上图可知，<b><font color='red'>随着模型复杂程度的增加，最终使得模型在训练集上表现的效果更好</font><strong>（训练集上的$Average Error$越来越小）</strong><font color='red'>而模型在测试集上结果坏掉的现象称为过拟合（Overfitting）。</font></b><br><br></p>
<h3 id="过拟合图形化解释"><a href="#过拟合图形化解释" class="headerlink" title="过拟合图形化解释"></a><strong>过拟合图形化解释</strong></h3><p>&emsp;&emsp;如下所示，假设每一个模型对应一个集合，可得出5次模型 $\supseteq$ 4次模型 $\supseteq$ 3次模型(原因是令高次项的 $w_i=0$ 可得到低次模型)。并且模型越复杂，在测试集上表现效果越好。<br><img src="https://s1.ax1x.com/2020/08/05/asXg3Q.png" alt="asXg3Q.png" border="0" style="zoom:67%;" / loading="lazy"></p>
<p>这5个函数在测试集上的表现如下：<br><img src="https://s1.ax1x.com/2020/08/05/asXRjs.png" alt="asXRjs.png" border="0" style="zoom:60%;"/ loading="lazy"></p>
<p>&emsp;&emsp;发现3次方以上的模型，已经出现了过拟合的现象，并且模型的泛化能力越来越差。所以考虑模型时要同时考虑训练集和测试集上$Average Error$。那么过拟合问题有没有办法解决呢？</p>
<br>

<h3 id="正则化解决过拟合问题"><a href="#正则化解决过拟合问题" class="headerlink" title="正则化解决过拟合问题"></a><strong>正则化解决过拟合问题</strong></h3><p>解决过拟合问题主要有两种方法，<font color='red'>法一是<strong>人工决定去掉某些特征</strong></font>（主要是针对引入特征过多造成过拟合），<font color='red'>法二是在<strong>保留原有特征同时采用正则化</strong></font>（主要针对模型复杂或引入特征过多造成过拟合）。以下主要介绍正则化解决过拟合问题。<br><br></p>
<h4 id="正则化概念"><a href="#正则化概念" class="headerlink" title="正则化概念"></a><strong>正则化概念</strong></h4><p>&emsp;&emsp;<b><font color='red'>正则化是指在保留原有特征的前提下，减小输入参数的幅值影响。</font></b>做法是在$Loss Function$中加入一个正则化参数$\lambda$，通过它来控制所有输入参数的权重。在本实验中的具体表达式为<br>$$ L=\sum_{n=1} (\hat y^n-(b+\omega x_{cp}^n))^2+\lambda\sum{(\omega_i^2)} $$<br><br></p>
<h4 id="能解决过拟合的原因"><a href="#能解决过拟合的原因" class="headerlink" title="能解决过拟合的原因"></a><strong>能解决过拟合的原因</strong></h4><p>&emsp;&emsp;过拟合产生的原因是整个模型过于依赖输入参数权值的影响。而正则化的思路是想办法降低参数权值的影响。</p>
<p>&emsp;&emsp;对于上述式子，$\lambda$的值可以人为调节。如果$\lambda$很小，那输出效果跟没添加一样。但是随着$\lambda$增大，在求同样的最小$Loss$的条件下，输入参数$\omega_i$的值会相对变小，从而抑制过拟合的产生。并且$\lambda$越大，$\omega_i$越小，拟合曲线越平滑，因为在直线中斜率值越小代表越不陡峭。为什么我们希望曲线越平滑越好呢？</p>
<p>&emsp;&emsp;假设模型为$y=b+\sum\omega_i x_i$，当输入一个微小的扰动$\Delta x_i$时，输出为$y=b+\sum\omega_i(x_i+\Delta x_i)$，此时当$\omega_i$的值足够小，则噪声点对于模型的影响就不明显了，从而降低了过拟合。但是如果$\lambda$值一直增大，会使得输出是一根近似于水平的直线，达不到拟合的要求，此时又陷入了“欠拟合”（underfitting）。<br><br></p>
<h4 id="正则化的效果"><a href="#正则化的效果" class="headerlink" title="正则化的效果"></a><strong>正则化的效果</strong></h4><p>&emsp;&emsp;对于同一个模型，进行正则化后调节$\lambda$在训练集和测试集的表现如下。<br><img src="https://s1.ax1x.com/2020/08/05/asXoNT.png" alt="asXoNT.png" border="0" style="zoom:67%;"/ loading="lazy"></p>
<p>&emsp;&emsp;可以看出，随着$\lambda$的值的增加，模型在测试集上的测试误差得到降低的同时训练误差也不会产生太大的变化。此时过拟合问题得到解决。但是当$\lambda$的值超过某一个数值后，测试误差又开始增加，此时又开始陷入了欠拟合。<br><br></p>
<h2 id="多特征的回归模型预测"><a href="#多特征的回归模型预测" class="headerlink" title="多特征的回归模型预测"></a><strong>多特征的回归模型预测</strong></h2><br>

<h3 id="加入宝可梦种类特征"><a href="#加入宝可梦种类特征" class="headerlink" title="加入宝可梦种类特征"></a><strong>加入宝可梦种类特征</strong></h3><p>&emsp;&emsp;这次我们在实验中抓取了60只宝可梦，它们的数据分布如下图所示。<br><img src="https://s3.ax1x.com/2020/11/15/DFP3tI.png" alt="DFP3tI.png" border="0" style="zoom:67%;" / loading="lazy"></p>
<p>此时若只考虑宝可梦的初始CP值这一个特征是远远不够的，我们猜测可能还跟宝可梦的种类有关。此时依然采用线性回归得到的模型为<br><img src="https://s1.ax1x.com/2020/08/05/asXfun.png" alt="asXfun.png" border="0" style="zoom:67%;" / loading="lazy"><br>此时又遇到一个新问题，怎样用一个函数将来表示这些不同物种的函数呢？如果你有学过信号与系统的话，就会知道有一个冲激函数$\delta(t-t_0)$，他的含义是当$t=t_0$时函数值为1，其余时刻为0。所以对应地我们这里令$t$等于各个物种即可。<br><img src="https://s1.ax1x.com/2020/08/05/asX4H0.jpg" alt="asX4H0.jpg" border="0" style="zoom: 80%;"/ loading="lazy"><br>该模型预测的结果为<br><img src="https://s1.ax1x.com/2020/08/05/asXhBq.png" alt="asXhBq.png" border="0" style="zoom: 67%;"/ loading="lazy"><br>由上图可知，宝可梦按种类划分在训练集上$Average Error$较小，而且预测的结果较为良好。<br><br></p>
<h3 id="加入宝可梦更多特征"><a href="#加入宝可梦更多特征" class="headerlink" title="加入宝可梦更多特征"></a><strong>加入宝可梦更多特征</strong></h3><p>&emsp;&emsp;在上图训练集中，还是有一些值在直线上上下扰动，拟合的不是很好。进化后的CP值会不会还和身高，体重和HP等特征有关呢？在不清楚的情况下，我们考虑把考虑到的特征全添加进去，重新建模如下所示。<br><img src="https://s3.ax1x.com/2020/11/15/DFi2Gt.png" alt="DFi2Gt.png" border="0" style="zoom:67%;"/ loading="lazy"><br>经过计算训练集和测试集上的$Average Error$发现又造成了过拟合,解决方法参考第四点哈哈哈。</p>
</div></section><div id="reward-container"><span class="hty-icon-button button-glow" id="reward-button" title="打赏" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === &quot;none&quot;) ? &quot;block&quot; : &quot;none&quot;;"><span class="icon iconify" data-icon="ri:hand-coin-line"></span></span><div id="reward-comment">来都来了，不点一下麽？(●ˇ∀ˇ●)</div><div id="qr" style="display:none;"><div style="display:inline-block"><a target="_blank" rel="noopener" href="https://joaquinchou-1259120139.cos.ap-guangzhou.myqcloud.com/img/img_config/alipay.jpg"><img loading="lazy" src="https://joaquinchou-1259120139.cos.ap-guangzhou.myqcloud.com/img/img_config/alipay.jpg" alt="支付宝" title="支付宝"></a><div><span style="color:#00A3EE"><span class="icon iconify" data-icon="ri:alipay-line"></span></span></div></div><div style="display:inline-block"><a target="_blank" rel="noopener" href="https://joaquinchou-1259120139.cos.ap-guangzhou.myqcloud.com/img/img_config/QQ.png"><img loading="lazy" src="https://joaquinchou-1259120139.cos.ap-guangzhou.myqcloud.com/img/img_config/QQ.png" alt="QQ 支付" title="QQ 支付"></a><div><span style="color:#12B7F5"><span class="icon iconify" data-icon="ri:qq-line"></span></span></div></div><div style="display:inline-block"><a target="_blank" rel="noopener" href="https://joaquinchou-1259120139.cos.ap-guangzhou.myqcloud.com/img/img_config/wechat.png"><img loading="lazy" src="https://joaquinchou-1259120139.cos.ap-guangzhou.myqcloud.com/img/img_config/wechat.png" alt="微信支付" title="微信支付"></a><div><span style="color:#2DC100"><span class="icon iconify" data-icon="ri:wechat-pay-line"></span></span></div></div></div></div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>Joaquin Chou</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="https://www.joaquinchou.com/academic_research/machine_learning/regression/" title="Regression">https://www.joaquinchou.com/academic_research/machine_learning/regression/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> 许可协议。</li></ul><script>document.addEventListener('copy', function (event) {
  const clipboardData = event.clipboardData || window.clipboardData;
  if (!clipboardData) { return; }
  const text = window.getSelection().toString();
  if (text) {
    event.preventDefault();
    clipboardData.setData('text/plain', text + '\n\n本文作者：Joaquin Chou\n本文链接：https://www.joaquinchou.com/academic_research/machine_learning/regression/\n版权声明：本博客所有文章除特别声明外，均默认采用 CC BY-NC-SA 4.0 许可协议。');
  }
});</script></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/academic_research/machine_learning/bias_and_variance/" rel="prev" title="Bias and Variance"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">Bias and Variance</span></a></div><div class="post-nav-item"></div></div></div><div class="hty-card" id="comment"><div class="comment-tooltip text-center"><span>可以点击下方进行评论哟！</span><br></div><div id="waline"></div><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@waline/client@v2/dist/waline.css"><script>window.CONFIG.waline.config.path = "/academic_research/machine_learning/regression/"</script><div class="js-Pjax"><script src="/js/comments/waline.js" type="module" defer></script></div></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2023 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> Joaquin Chou</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v6.3.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.11</span></div><div class="live-time"><span>本博客已萌萌哒地运行</span><span id="display_live_time"></span><span class="moe-text">(●'◡'●)</span><script>function blog_live_time() {
  setTimeout(blog_live_time, 1000);
  const start = new Date('2020-11-12T00:00:00');
  const now = new Date();
  const timeDiff = (now.getTime() - start.getTime());
  const msPerMinute = 60 * 1000;
  const msPerHour = 60 * msPerMinute;
  const msPerDay = 24 * msPerHour;
  const passDay = Math.floor(timeDiff / msPerDay);
  const passHour = Math.floor((timeDiff % msPerDay) / 60 / 60 / 1000);
  const passMinute = Math.floor((timeDiff % msPerHour) / 60 / 1000);
  const passSecond = Math.floor((timeDiff % msPerMinute) / 1000);
  display_live_time.innerHTML = ` ${passDay} 天 ${passHour} 小时 ${passMinute} 分 ${passSecond} 秒`;
}
blog_live_time();
</script></div><div id="busuanzi"><span id="busuanzi_container_site_uv" title="总访客量"><span><span class="icon iconify" data-icon="ri:user-line"></span></span><span id="busuanzi_value_site_uv"></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv" title="总访问量"><span><span class="icon iconify" data-icon="ri:eye-line"></span></span><span id="busuanzi_value_site_pv"></span></span><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#9400D3" stroke-width="2" stroke-linecap="round"></circle></svg></a><a class="popup-trigger hty-icon-button icon-search" id="search" href="javascript:;" title="搜索"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:search-line"></span></span></a><script>window.addEventListener("DOMContentLoaded", () => {
  // Handle and trigger popup window
  document.querySelector(".popup-trigger").addEventListener("click", () => {
    document.querySelector(".popup").classList.add("show");
    setTimeout(() => {
      document.querySelector(".search-input").focus();
    }, 100);
  });

  // Monitor main search box
  const onPopupClose = () => {
    document.querySelector(".popup").classList.remove("show");
  };

  document.querySelector(".popup-btn-close").addEventListener("click", () => {
    onPopupClose();
  });

  window.addEventListener("keyup", event => {
    if (event.key === "Escape") {
      onPopupClose();
    }
  });
});
</script><script defer src="https://fastly.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script defer src="https://fastly.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script defer src="/js/search/algolia-search.js" type="module"></script><div class="popup search-popup"><div class="search-header"><span class="popup-btn-close close-icon hty-icon-button"><span class="icon iconify" data-icon="ri:close-line"></span></span></div><div class="search-input-container"></div><div class="algolia-results"><div id="algolia-stats"></div><div id="algolia-hits"></div><div class="algolia-pagination" id="algolia-pagination"></div></div></div><script>function initMourn() {
  const date = new Date();
  const today = (date.getMonth() + 1) + "-" + date.getDate()
  const mourn_days = ["4-4","9-18","3-3"]
  if (mourn_days.includes(today)) {
    document.documentElement.style.filter = "grayscale(1)";
  }
}
initMourn();</script></body></html>