<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#9400D3"><meta name="author" content="Joaquin Chou"><meta name="copyright" content="Joaquin Chou"><meta name="generator" content="Hexo 6.3.0"><meta name="theme" content="hexo-theme-yun"><title>Logistic Regression | 喵语小站</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...true?.options,
  });
});</script><link rel="icon" type="image/png" href="/favicon.ico"><link rel="mask-icon" href="/favicon.ico" color="#9400D3"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"www.joaquinchou.com","root":"/","title":"喵星暖暖窝","version":"1.10.11","mode":"time","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"algolia":{"appID":"L0D85KPS1S","apiKey":"715edff2e91248577c19b580f3efcb3a","indexName":"prod_NAME","hits":{"per_page":8}},"fireworks":{"colors":["0,250,154","255,69,0","33, 78, 194"]},"waline":{"config":{"enable":true,"serverURL":"https://blog-api-joaquinchou.vercel.app","comment":false,"el":"#waline","lang":"zh-CN"},"cdn":"https://fastly.jsdelivr.net/npm/@waline/client@v2/dist/waline.js","dark":"html.dark"},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><script>(function(){
  var bp = document.createElement('script');
  var curProtocol = window.location.protocol.split(':')[0];
  if (curProtocol === 'https') {
    bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
  }
  else {
    bp.src = 'http://push.zhanzhang.baidu.com/push.js';
  }
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(bp, s);
})();</script><meta name="description" content="&amp;emsp;&amp;emsp;这应该是李宏毅机器学习入门基础概念的最后一篇了，后面就因为偷懒◑﹏◐开学去读论文，学习经典的深度学习模型就没写了。其实逻辑回归是构成深度学习神经元的激活函数，虽然说为了防止梯度爆炸和梯度消失，现在普遍使用的是Relu，但其重要性还是无与伦比。">
<meta property="og:type" content="article">
<meta property="og:title" content="Logistic Regression">
<meta property="og:url" content="https://www.joaquinchou.com/academic_research/machine_learning/logistic_regression/index.html">
<meta property="og:site_name" content="喵语小站">
<meta property="og:description" content="&amp;emsp;&amp;emsp;这应该是李宏毅机器学习入门基础概念的最后一篇了，后面就因为偷懒◑﹏◐开学去读论文，学习经典的深度学习模型就没写了。其实逻辑回归是构成深度学习神经元的激活函数，虽然说为了防止梯度爆炸和梯度消失，现在普遍使用的是Relu，但其重要性还是无与伦比。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/16/dE0Lp6.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/16/dE0Ht1.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/16/dE05m4.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/18/dnvj0I.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/16/dE0jXD.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/16/dE0z0H.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/16/dEBS7d.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/16/dEB9AA.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/16/dEBPht.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/16/dEBF9P.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/16/dEBm7j.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/16/dEBk1f.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/16/dEBAc8.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/16/dEBEjS.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/16/dEBuAs.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/16/dEB3cT.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/16/dEBl90.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/16/dEBtHJ.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/16/dEBaNR.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/23/d0ugje.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/23/d0nFRU.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/25/d6wpsU.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/23/d0nkzF.jpg">
<meta property="article:published_time" content="2020-08-24T16:00:00.000Z">
<meta property="article:modified_time" content="2022-03-06T15:34:58.488Z">
<meta property="article:author" content="Joaquin Chou">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s1.ax1x.com/2020/08/16/dE0Lp6.jpg"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><canvas id="trianglifyContainer"></canvas><script defer src="https://fastly.jsdelivr.net/npm/trianglify@4/dist/trianglify.bundle.js"></script><script>document.addEventListener("DOMContentLoaded", () => {
  const pattern = trianglify({
    width: 800,
    height: 600,
    cellSize: 75,
    palette: ["Oranges", "RdBu", "Purples", "Blues"],
  });
  const canvasOpts = {
    applyCssScaling: false
  }
  document.body.appendChild(pattern.toCanvas(trianglifyContainer, canvasOpts));
});</script><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="Joaquin Chou"><img width="96" loading="lazy" src="https://joaquinchou-1259120139.cos.ap-guangzhou.myqcloud.com/img/img_config/JTpLkT.jpg" alt="Joaquin Chou"></a><div class="site-author-name"><a href="/about/">Joaquin Chou</a></div><a class="site-name" href="/about/site.html">喵语小站</a><sub class="site-subtitle">Hope that someone you love would also love you!</sub><div class="site-description">希望你喜欢的人她/他也爱你！</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">17</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">4</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">8</span></a></div><a class="site-state-item hty-icon-button" href="/404.html" title="还没想好呢"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/JoaquinChou" title="GitHub" target="_blank" style="color:#FF00FF"><span class="icon iconify" data-icon="ri:github-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://music.163.com/#/my/m/music/playlist?id=621567138" title="网易云音乐" target="_blank" style="color:#C20C0C"><span class="icon iconify" data-icon="ri:netease-cloud-music-line"></span></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="我的小伙伴们" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a><a class="links-item hty-icon-button" href="/albums/" title="我的相册" style="color:#DA70D6"><span class="icon iconify" data-icon="ri:gallery-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%AE%9E%E7%8E%B0%E8%BF%87%E7%A8%8B"><span class="toc-number">2.</span> <span class="toc-text">逻辑回归实现过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BB%BA%E6%A8%A1%EF%BC%88%E6%89%BE%E6%9C%AA%E5%AE%9A%E5%8F%82%E5%87%BD%E6%95%B0%EF%BC%89"><span class="toc-number">2.1.</span> <span class="toc-text">建模（找未定参函数）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%BE%E8%AF%84%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="toc-number">2.2.</span> <span class="toc-text">找评价函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5"><span class="toc-number">2.2.1.</span> <span class="toc-text">交叉熵</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="toc-number">2.2.1.1.</span> <span class="toc-text">交叉熵的定义</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%BE%E8%AF%84%E4%BB%B7%E6%9C%80%E5%A5%BD%E7%9A%84%E5%87%BD%E6%95%B0"><span class="toc-number">2.3.</span> <span class="toc-text">找评价最好的函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%92%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BD%9C%E6%AF%94%E8%BE%83"><span class="toc-number">3.</span> <span class="toc-text">逻辑回归和线性回归作比较</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%BD%9C%E6%AF%94%E8%BE%83"><span class="toc-number">4.</span> <span class="toc-text">判别模型和生成模型作比较</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Softmax%E5%9B%9E%E5%BD%92"><span class="toc-number">5.</span> <span class="toc-text">Softmax回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%B1%80%E9%99%90"><span class="toc-number">6.</span> <span class="toc-text">逻辑回归的局限</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E8%BD%AC%E6%8D%A2"><span class="toc-number">6.1.</span> <span class="toc-text">特征转换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%A7%E8%81%94%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-number">6.2.</span> <span class="toc-text">级联逻辑回归模型</span></a></li></ol></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#9400D3;"><link itemprop="mainEntityOfPage" href="https://www.joaquinchou.com/academic_research/machine_learning/logistic_regression/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="Joaquin Chou"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="喵语小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Logistic Regression</h1><div class="post-meta"><div class="post-time" style="display:block"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="创建时间：2020-08-25 00:00:00" itemprop="dateCreated datePublished" datetime="2020-08-25T00:00:00+08:00">2020-08-25</time></div><span class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><span class="icon iconify" data-icon="ri:file-word-line"></span></span> <span title="本文字数">4.5k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><span class="icon iconify" data-icon="ri:timer-line"></span></span> <span title="阅读时长">16m</span></span></span><span class="post-busuanzi"><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读次数"><span class="icon iconify" data-icon="ri:eye-line"></span> <span id="busuanzi_value_page_pv"></span></span></span><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/%E5%96%B5%E8%AF%AD%E7%AC%94%E8%AE%B0/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">喵语笔记</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="--text-color:#3776ab"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">机器学习</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><p>&emsp;&emsp;这应该是李宏毅机器学习入门基础概念的最后一篇了，后面就因为<del>偷懒◑﹏◐</del>开学去读论文，学习经典的深度学习模型就没写了。其实逻辑回归是构成深度学习神经元的激活函数，虽然说为了防止梯度爆炸和梯度消失，现在普遍使用的是Relu，但其重要性还是无与伦比。</p>
<span id="more"></span>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a><b>概述</b></h2><p>&emsp;&emsp;虽然逻辑回归（Logistic Regression）被称为回归，但它一般用于解决分类问题。为什么叫逻辑回归呢？因为在整个方法中对输入$x$我们采用线性模型$z=\omega^Tx+b$，线性模型可以进行回归学习，如果处理的是二分类问题，我们规定输出$z={0,1}$，所以叫逻辑回归。</p>
<h2 id="逻辑回归实现过程"><a href="#逻辑回归实现过程" class="headerlink" title="逻辑回归实现过程"></a><b>逻辑回归实现过程</b></h2><p>&emsp;&emsp;由概述我们已知知道逻辑回归是采用线性模型，假设我们现在要解决二分类问题。那怎么将模型$z=\omega^Tx+b$映射到输出$z={0,1}$呢？这时候就要利用我们在Classification最后由贝叶斯公式推导出来的$P_{\omega,b}(C_1|x)=$Sigmoid函数$\sigma(z)=\frac1{1+e^{-z} }$，具体做法如下。</p>
<h3 id="建模（找未定参函数）"><a href="#建模（找未定参函数）" class="headerlink" title="建模（找未定参函数）"></a><b>建模（找未定参函数）</b></h3><p>&emsp;&emsp;建模的过程就是找出一系列参数未定的函数列。对于二分类问题，我们的想法是通过与$\sigma(z)$函数相等的后验概率$P_{\omega,b}(C_1|x)$来进行判别，如果后验概率$P_{\omega,b}(C_1|x)$大于0.5，则属于类别$C_1$，小于0.5则属于类别$C_2$。所以整个模型表达式为<br>$$ f_{\omega,b}(x)=P_{\omega,b}(C_1|x)=\sigma(z) $$<br>由在Classification中的描述可知，$z=\omega\cdot x+b$（$\omega, x$都是列向量），所以对于上式，$\omega,b$是未定的参数。$f_{\omega,b}(x)$是由这两个未定参数组成的函数列。</p>
<p>&emsp;&emsp;如果我们把整个模型画出可视化图的形式，结果如下所示。</p>
<img src="https://s1.ax1x.com/2020/08/16/dE0Lp6.jpg" alt="dE0Lp6.jpg" border="0" / loading="lazy">

<h3 id="找评价函数"><a href="#找评价函数" class="headerlink" title="找评价函数"></a><b>找评价函数</b></h3><p>&emsp;&emsp;确定模型后，我们需要定义一个判断模型好坏的评价标准，目前为止的评价函数有损失函数和似然函数。目前只有定义了<b>损失函数(Loss Function)或者似然函数(Likelihood Function)</b>，我们才能用各种求最优解的方法去求出使得损失最小或者似然函数最大的参数。</p>
<p>&emsp;&emsp;在逻辑回归中，不妨我们先用似然函数去作为评价标准。假设现在我们现在处理二分类问题，训练集是已知分类的$N$个样本点，利用上述模型和独立事件的最大似然估计的方法，我们可以得到似然函数$L(\omega,b)$的表达式如下。</p>
<img src="https://s1.ax1x.com/2020/08/16/dE0Ht1.jpg" alt="dE0Ht1.jpg" border="0" / loading="lazy">

<p>其中设$\omega^*,b^*$是使得似然函数值最大的参数，解函数为$\omega^*,b^*= \underset{\omega,b}{\operatorname{arg,max}L(\omega,b)}$，直接求最大解不好求，我们可以改成求最小值的形式为$\omega^*,b^*= \underset{\omega,b}{\operatorname{arg,min}}-lnL(\omega,b)$，类别$C_1$我们用1标注，类别$C_2$我们用0标注。则似然函数$L(\omega,b)$的每一项都可以化成下面的结果。</p>
<img src="https://s1.ax1x.com/2020/08/16/dE05m4.jpg" alt="dE05m4.jpg" border="0" / loading="lazy">

<p>（注：图中右上角的标注错误，应该是$\hat y_1=1,\hat y_2=1,\hat y_3=0$）</p>
<h4 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a><b>交叉熵</b></h4><p>&emsp;&emsp;那为什么我们要转化成对数加负号求最小值的形式计算解函数？因为取对数加负号后，可以转化为《信息论》中常见的量——<b>熵（entropy）</b>。为什么要转化为熵呢？因为熵是用来系统的混乱程度，熵值越大，代表系统越混乱，系统里所含的样本种类数越多；熵值越小，系统越纯，系统里面的样本种类数越单一。</p>
<p>&emsp;&emsp;熵的数学定义为$H(x)=−∑_{i=1}^np(x_i)log(p(x_i))$；其中，$p(x_i)$为事件$X=x_i$的概率，$−log(p(x_i))$为事件$X=x_i$信息量。</p>
<h5 id="交叉熵的定义"><a href="#交叉熵的定义" class="headerlink" title="交叉熵的定义"></a><b>交叉熵的定义</b></h5><p>&emsp;&emsp;根据熵的定义我们知道熵可以通过自身的概率分布，衡量自身的混乱程度。那对于两个不同的分布$p,q$，使用熵的定义去衡量它们之间的混乱程度，也就是它们之间的差异，可得到交叉熵的定义为<br>$$ H(p,q)=-\sum_x p(x)ln(q(x)) $$<br>其中交叉熵值越大，代表两个分布差异很大，交叉熵值越小，代表两个分布越相似。</p>
<img src="https://s1.ax1x.com/2020/08/18/dnvj0I.jpg" alt="dnvj0I.jpg" border="0" / loading="lazy">

<p>&emsp;&emsp;对应到机器学习中，可以用<font color='red'><b>交叉熵作为损失函数</b></font>，假设分布$p$表示真实的分布，分布$q$表示训练后的模型预测标记分布，此时交叉熵可以衡量二者的差异性，交叉熵值越小，去负号去指数得到的似然函数的值越大，可求得最优解。</p>
<p>&emsp;&emsp;上图中划蓝线的式子代表的是两个伯努利分布（二项分布）的交叉熵。</p>
<h3 id="找评价最好的函数"><a href="#找评价最好的函数" class="headerlink" title="找评价最好的函数"></a><b>找评价最好的函数</b></h3><p>&emsp;&emsp;我们已经知道可以用两个不同分布的交叉熵去作为损失函数去衡量函数好坏。所以我们的任务是求出使得交叉熵的值最小的$\omega,b$，以下使用梯度下降的方法求解。（梯度下降具体的做法参照Regression）</p>
<p>&emsp;&emsp;下图是$lnf_{\omega,b}(x^n)$对$\omega_i$求偏导的过程。</p>
<img src="https://s1.ax1x.com/2020/08/16/dE0jXD.jpg" alt="dE0jXD.jpg" border="0" / loading="lazy">

<p>以下是$ln(1-f_{\omega,b}(x^n))$对$\omega_i$求偏导的过程。</p>
<img src="https://s1.ax1x.com/2020/08/16/dE0z0H.jpg" alt="dE0z0H.jpg" border="0" / loading="lazy">

<p>将两项求偏导的结果加起来化简如下。</p>
<img src="https://s1.ax1x.com/2020/08/16/dEBS7d.jpg" alt="dEBS7d.jpg" border="0" / loading="lazy">

<p>可以得到$\omega_i$更新为$\omega_i-\eta\sum_n-(\hat y^n-f_{\omega,b}(x^n)x_i^n)$。其中$\eta$为学习率。</p>
<h2 id="逻辑回归和线性回归作比较"><a href="#逻辑回归和线性回归作比较" class="headerlink" title="逻辑回归和线性回归作比较"></a><b>逻辑回归和线性回归作比较</b></h2><p>&emsp;&emsp;两者的对比图如下所示。</p>
<img src="https://s1.ax1x.com/2020/08/16/dEB9AA.jpg" alt="dEB9AA.jpg" border="0" / loading="lazy">

<p>可以看出，两种回归的$\omega_i$的更新是一样的，不同的是，线性回归的目标函数可以是任意值，输出也可以是任意值，逻辑回归的目标函数是0或1，输出是介于0到1的值。并且线性回归的损失函数采用的是平方误差，而逻辑回归采用的是交叉熵。那为什么逻辑回归不采用平方误差作为损失函数呢？</p>
<p>&emsp;&emsp;其实损失函数只是个人为定义评价模型好坏的标准而已。不同的标准各有优劣，但是并不冲突，所以逻辑回归当然可以利用平方误差得到损失函数。但是可能存在以下问题。如果从分类问题的角度去解释，由下图可知，如果我们分类的目标值$\hat y^n=1$，当我们的预测值在距离目标很近（$f_{\omega,b}(x^n)=1$）或是很远（$f_{\omega,b}(x^n)=0）$时，似然函数$L$关于$\omega_i$的偏导都是0，换言之，<b>当预测值在距离目标值很远的地方时，可能陷入局部最优解而不进行参数的更新或者更新很慢。</b></p>
<img src="https://s1.ax1x.com/2020/08/16/dEBPht.jpg" alt="dEBPht.jpg" border="0" / loading="lazy">

<p>如果我们分类的目标值$\hat y^n=0$，同样会存在上述问题，具体过程如下。</p>
<img src="https://s1.ax1x.com/2020/08/16/dEBF9P.jpg" alt="dEBF9P.jpg" border="0" / loading="lazy">

<p>&emsp;&emsp;如果从数学的角度解释，是因为通过最小二乘法得到的平方损失函数是非凸的，而对数似然函数是凸函数。二者的图像如下所示。</p>
<img src="https://s1.ax1x.com/2020/08/16/dEBm7j.jpg" alt="dEBm7j.jpg" border="0" / loading="lazy">

<p>可见如果是交叉熵，在预测值距离目标越远，微分值就缩小，就可以做到距离目标越远，更新参数越快。而平方误差在距离目标很远的时候，微分值也可能非常小，这会造成移动的速度非常慢，效果较差。</p>
<p>&emsp;&emsp;还记得为什么在线性回归中我们不用担心陷入局部最优解的问题而选择平方误差作为损失函数吗？<font color='red'>因为线性回归中，我们的损失函数$L$根本没有局部最优解。（因为无论是单变量还是多变量的线性回归，损失函数$L$都是凸函数。）</font></p>
<h2 id="判别模型和生成模型作比较"><a href="#判别模型和生成模型作比较" class="headerlink" title="判别模型和生成模型作比较"></a><b>判别模型和生成模型作比较</b></h2><p>&emsp;&emsp;逻辑回归的方法就属于判别模型，就是通过$\sigma(z)$函数建模，通过构造对数似然函数作为评价函数，再利用梯度下降的方法直接求解最好的$\omega,b$。如果是生成模型，要已知或者先假设训练数据服从某一分布（像上节在Classification中的概率生成模型中假设原数据服从高斯分布），再通过最大似然估计求出最好的参数（高斯分布的参数是$\mu^1,\mu^2,\Sigma$，再通过训练分类样本数$N_1,N_2$等参数求解得到最好的$w,b$。）所以对于判别模型和生成模型，它们得到的最好的$\omega,b$显然是不一样的。</p>
<img src="https://s1.ax1x.com/2020/08/16/dEBk1f.jpg" alt="dEBk1f.jpg" border="0" / loading="lazy">

<p>如果我们使用逻辑回归对宝可梦的水系和一般系的作分类，只用两个特征的话，和之前生成模型的方法对比，效果如下。</p>
<img src="https://s1.ax1x.com/2020/08/16/dEBAc8.jpg" alt="dEBAc8.jpg" border="0" / loading="lazy">

<p>只考虑两个特征效果确实也不怎么好，如果我们考虑了所有的7个特征，准确率由73%提高到了79%。所以判别模型的效果比生成模型的要好。那判别模型的方法一定会比生成模型要好呢？</p>
<p>&emsp;&emsp;以下举个例子来说明，下图中有13组数据，除了（1,1)是类别1外，其他都是类别2。</p>
<img src="https://s1.ax1x.com/2020/08/16/dEBEjS.jpg" alt="dEBEjS.jpg" border="0" / loading="lazy">

<p>然后如果我们新来个测试数据（1,1），毫无意外我们都认为是类别1，但是如果我们选用生成模型——<b>朴素贝叶斯分类器（Naive Bayes Classifier）</b>（假设问题的输入有$K$个特征，并且它们之间都是统计独立，最终先验概率的求解为各个特征的概率相乘。）进行判别，结果会怎样呢？</p>
<p>&emsp;&emsp;为使用贝叶斯公式，我们先求出贝叶斯公式的四个先验概率，再带入贝叶斯公式，求得的结果$P(C_1|x)$是小于0.5，所以分类器将它判断为类别2，这和我们的判断恰好相反。为什么使用生成模型会造成这种结果呢？</p>
<p>&emsp;&emsp;因为生成模型在进行判断前对我们的数据做了假设，假设它来自于某个概率模型（本例就是朴素贝叶斯分类器），在数据量不大的情况下进行“脑补”，当“脑补”这个模型的基础上产生足够多的数据的话，（1,1）属于类别2的概率更大，之所以我们看到（1,1）属于类别1，是因为训练的数据不够多。</p>
<img src="https://s1.ax1x.com/2020/08/16/dEBuAs.jpg" alt="dEBuAs.jpg" border="0" / loading="lazy">

<p>那生成模型中的“脑补”算是一件好事吗？在数据量不多的情况下，“脑补”确实是必要的。那生成模型的优点有哪些呢？</p>
<p>&emsp;&emsp;<b>对于训练数据很少或者训练集中含有噪声点时，使用生成模型往往更好，因为判别模型没有自己的假设，受数据影响较大。当数据量越来越多时，使用判别模型的$Error$会越来越小。使用生成模型还可以将先验和类相关的概率从不同的来源估计。</b>比如说语音识别，我们可能直观会认为现在的语音识别大都使用神经网络来进行处理，是属于判别模型；但实际上整个语音识别是生成模型。所以还是需要算一个先验概率——某句话被说出来的概率，但我们估计某句话被说出来的概率不需要声音数据，只需要网络上爬很多的句子，就能计算某句话出现的几率。</p>
<h2 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a><b>Softmax回归</b></h2><p>&emsp;&emsp;普通的逻辑回归只能解决二分类问题，对于多分类问题，我们对逻辑回归进行改进提出了Softmax回归。关于Softmax回归的原理，可参考《Pattern Recognition and Machine Learning》Christopher M. Bishop 著 ，P209-210。以下我们只介绍Softmax回归的用法。</p>
<p>&emsp;&emsp;假设我们现在有三个类别$C_1,C_2,C_3$（记为$    C_i(i=1,2,3)$），它们被预测时都有回归中最好的权值$w_i$和偏置$b_i$，假设现在有测试样本$x$测试集输入模型后对应的输出是$z_i(i=1,2,3)$，而Softmax要做的是利用以下公式计算样本$x$的后验概率$y_i=P(C_i|x)(i=1,2,3)$。后验概率的计算使用公式<br>$$ y_i= \frac{e^{z_1}} {\sum_{j=1}^3 e^{z_j}} $$<br>具体的计算过程如下。Softmax回归的过程简单描述为将样本$x$输入到各个判别器得到<font color='red'>输出$z$作<b>指数化（exponential）</b></font>，但由于指数化的结果可以是任意值，所以还要各个判别器<font color='red'>指数化结果的<b>归一化（Normalization）</b></font>。</p>
<img src="https://s1.ax1x.com/2020/08/16/dEB3cT.jpg" alt="dEB3cT.jpg" border="0" / loading="lazy">

<p>假设我们输入样本$x$经过线性回归模型后三个输出为$z_1=3,z_2=1.z_3=-3$，再经过Softmax回归处理，得到的结果是$y_1=0.88,y_2=0.12,y_3=0$，换句话说，<font color='red'>Softmax回归的输出可以当作后验概率</font>。</p>
<p>&emsp;&emsp;那为什么我们可以把Softmax回归的输出可以当作后验概率呢？</p>
<p>&emsp;&emsp;<font color='green'>首先，我们可以像在Classification中一样，假设多个类别都服从高斯分布，然后让它们共享协方差矩阵$\Sigma$，再用最大似然估计的方法求得$\mu_1,\mu_2,\mu_3$，最后也能求得$y_1,y_2,y_3$。其次，在《信息论》中有个叫“最大熵”的概念，一样能推导出Softmax。（参考：指数簇分布的最大熵等价于其指数形式的最大似然界。二项式的最大熵解等价于二项式指数形式(sigmoid)的最大似然，多项式分布的最大熵等价于多项式分布指数形式(softmax)的最大似然，因此为什么用sigmoid函数，那是因为指数簇分布最大熵的特性的必然性。）</font></p>
<p>&emsp;&emsp;如下图所示，最后为避免使用数值1，2，3进行分类我们采用矩阵表示，再用多个变量的交叉熵作为损失函数去衡量目标值和预测值的差距。</p>
<img src="https://s1.ax1x.com/2020/08/16/dEBl90.jpg" alt="dEBl90.jpg" border="0" / loading="lazy">

<h2 id="逻辑回归的局限"><a href="#逻辑回归的局限" class="headerlink" title="逻辑回归的局限"></a><b>逻辑回归的局限</b></h2><p>&emsp;&emsp;其实逻辑回归处理分类问题有个非常强的局限，就是它根本没有办法将异或问题分类，如下图所示。</p>
<img src="https://s1.ax1x.com/2020/08/16/dEBtHJ.jpg" alt="dEBtHJ.jpg" border="0" / loading="lazy">

<p>无论我们怎样去调整模型的权重$w_i$还是偏置$b$，模型仍是一条不能将二者区分开来的直线，如下图所示。</p>
<img src="https://s1.ax1x.com/2020/08/16/dEBaNR.jpg" alt="dEBaNR.jpg" border="0" / loading="lazy">

<p>&emsp;&emsp;那我们应该怎么解决这个问题呢？</p>
<h3 id="特征转换"><a href="#特征转换" class="headerlink" title="特征转换"></a><b>特征转换</b></h3><p>&emsp;&emsp;异或问题难以分类的原因是分类的特征实在过于特殊，如果我们对原始特征进行<b><font color='red'>特征转换（Feature Transformation）</font></b>就能解决这个问题。特征转换的方法有很多种，对于上述01异或问题，可以将两个特征组成的平面$[x_1,x_2]^T$转化为新特征的平面$[x_1^\prime,x_2^\prime]^T$，其中$x_1^\prime$代表某点距离蓝点$[0,0]^T$的距离，$x_1^\prime$代表某点距离蓝点$[1,1]^T$的距离。所以对于$[x_1^\prime,x_2^\prime]^T$，$[1,1]^T$表示在原特征$x_1,x_2$的坐标平面中距离两个蓝点的距离都是1的点，也就是两个红点；对于原来的蓝点$[0,0]$，如果$x_1^\prime=0$，则$x_2^\prime=\sqrt{2}$，对于原来的蓝点$[1,1]$，如果$x_2^\prime=0$，则$x_1^\prime=\sqrt{2}$。</p>
<p>&emsp;&emsp;所以可以得到新平面下的两种类别点的分布，此时可以用逻辑回归将二者区分开来。</p>
<img src="https://s1.ax1x.com/2020/08/23/d0ugje.jpg" alt="d0ugje.jpg" border="0" / loading="lazy">

<p>以上特征转换的方法只是针对上述的问题而已，但实际上我们不一定总能像刚刚那样找出一个好的特征转换。那怎么对不好的特征值进行特征转换呢？</p>
<h3 id="级联逻辑回归模型"><a href="#级联逻辑回归模型" class="headerlink" title="级联逻辑回归模型"></a><b>级联逻辑回归模型</b></h3><p>&emsp;&emsp;实际上我们可以通过级联多个逻辑回归模型进行特征转换，再通过最后一个逻辑回归模型进行分类。如下图所示。</p>
<img src="https://s1.ax1x.com/2020/08/23/d0nFRU.jpg" alt="d0nFRU.jpg" border="0" / loading="lazy">

<p>&emsp;&emsp;下面将以异或问题为例说明具体如何操作。<b>假设我们有四个待分类异或样本，每个样本有两个特征$x_1,x_2$，我们要做的事是通过逻辑回归进行特征转换，将不利于分类的特征转换为利于分类的特征，再将利于分类的特征通过一个独立的逻辑回归进行分类。</b></p>
<p>&emsp;&emsp;在特征转换步骤中有$z_1=\omega_1x_1+\omega_2x_2+b_1$和$z_2=\omega_3x_1+\omega_4x_2+b_2$，得到$z_1,z_2$再输入$\sigma(z)$函数得到归一化后的新的特征$x^\prime_1$和$x^\prime_2$，我们得到的新的特征是可以通过逻辑回归进行分类的。</p>
<p>&emsp;&emsp;所以在分类步骤中先进行$z_3=\omega_5x_1^\prime+\omega_6x_2^\prime+b_3$，通过调整参数得到输出再输入$\sigma(z)$函数得到归一化后的输出$y$也就是后验概率，再根据后验概率是否大于0.5进行分类。如下图，通过调节权重和偏置的参数可以使得四个待分类样本的$x_1^\prime,x_2^\prime$值如下所示。</p>
<img src="https://s1.ax1x.com/2020/08/25/d6wpsU.jpg" alt="d6wpsU.jpg" border="0" / loading="lazy">

<p>由上可见，一个逻辑回归的输入可以来自其他逻辑回归的输出，同时该逻辑回归的输出也可以作为其他逻辑回归的输入，组成的网络如下图所示。</p>
<img src="https://s1.ax1x.com/2020/08/23/d0nkzF.jpg" alt="d0nkzF.jpg" border="0" / loading="lazy">

<p>由多个逻辑回归组成的网络称为<b><font color='red'>“类神经网络”（Neural Network）</font></b>，其中每个逻辑回归叫做<b><font color='red'>神经元（Neural）</font></b>。这就引出了以后要讲的深度学习（Deep Learning）。</p>
</div></section><div id="reward-container"><span class="hty-icon-button button-glow" id="reward-button" title="打赏" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === &quot;none&quot;) ? &quot;block&quot; : &quot;none&quot;;"><span class="icon iconify" data-icon="ri:hand-coin-line"></span></span><div id="reward-comment">来都来了，不点一下麽？(●ˇ∀ˇ●)</div><div id="qr" style="display:none;"><div style="display:inline-block"><a target="_blank" rel="noopener" href="https://joaquinchou-1259120139.cos.ap-guangzhou.myqcloud.com/img/img_config/alipay.jpg"><img loading="lazy" src="https://joaquinchou-1259120139.cos.ap-guangzhou.myqcloud.com/img/img_config/alipay.jpg" alt="支付宝" title="支付宝"></a><div><span style="color:#00A3EE"><span class="icon iconify" data-icon="ri:alipay-line"></span></span></div></div><div style="display:inline-block"><a target="_blank" rel="noopener" href="https://joaquinchou-1259120139.cos.ap-guangzhou.myqcloud.com/img/img_config/QQ.png"><img loading="lazy" src="https://joaquinchou-1259120139.cos.ap-guangzhou.myqcloud.com/img/img_config/QQ.png" alt="QQ 支付" title="QQ 支付"></a><div><span style="color:#12B7F5"><span class="icon iconify" data-icon="ri:qq-line"></span></span></div></div><div style="display:inline-block"><a target="_blank" rel="noopener" href="https://joaquinchou-1259120139.cos.ap-guangzhou.myqcloud.com/img/img_config/wechat.png"><img loading="lazy" src="https://joaquinchou-1259120139.cos.ap-guangzhou.myqcloud.com/img/img_config/wechat.png" alt="微信支付" title="微信支付"></a><div><span style="color:#2DC100"><span class="icon iconify" data-icon="ri:wechat-pay-line"></span></span></div></div></div></div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>Joaquin Chou</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="https://www.joaquinchou.com/academic_research/machine_learning/logistic_regression/" title="Logistic Regression">https://www.joaquinchou.com/academic_research/machine_learning/logistic_regression/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> 许可协议。</li></ul><script>document.addEventListener('copy', function (event) {
  const clipboardData = event.clipboardData || window.clipboardData;
  if (!clipboardData) { return; }
  const text = window.getSelection().toString();
  if (text) {
    event.preventDefault();
    clipboardData.setData('text/plain', text + '\n\n本文作者：Joaquin Chou\n本文链接：https://www.joaquinchou.com/academic_research/machine_learning/logistic_regression/\n版权声明：本博客所有文章除特别声明外，均默认采用 CC BY-NC-SA 4.0 许可协议。');
  }
});</script></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/academic_research/anomaly_detection/MemAE/" rel="prev" title="MemAE"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">MemAE</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/academic_research/machine_learning/classification/" rel="next" title="Classification"><span class="post-nav-text">Classification</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"><div class="comment-tooltip text-center"><span>可以点击下方进行评论哟！</span><br></div><div id="waline"></div><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@waline/client@v2/dist/waline.css"><script>window.CONFIG.waline.config.path = "/academic_research/machine_learning/logistic_regression/"</script><div class="js-Pjax"><script src="/js/comments/waline.js" type="module" defer></script></div></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2023 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> Joaquin Chou</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v6.3.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.11</span></div><div class="live-time"><span>本博客已萌萌哒地运行</span><span id="display_live_time"></span><span class="moe-text">(●'◡'●)</span><script>function blog_live_time() {
  setTimeout(blog_live_time, 1000);
  const start = new Date('2020-11-12T00:00:00');
  const now = new Date();
  const timeDiff = (now.getTime() - start.getTime());
  const msPerMinute = 60 * 1000;
  const msPerHour = 60 * msPerMinute;
  const msPerDay = 24 * msPerHour;
  const passDay = Math.floor(timeDiff / msPerDay);
  const passHour = Math.floor((timeDiff % msPerDay) / 60 / 60 / 1000);
  const passMinute = Math.floor((timeDiff % msPerHour) / 60 / 1000);
  const passSecond = Math.floor((timeDiff % msPerMinute) / 1000);
  display_live_time.innerHTML = ` ${passDay} 天 ${passHour} 小时 ${passMinute} 分 ${passSecond} 秒`;
}
blog_live_time();
</script></div><div id="busuanzi"><span id="busuanzi_container_site_uv" title="总访客量"><span><span class="icon iconify" data-icon="ri:user-line"></span></span><span id="busuanzi_value_site_uv"></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv" title="总访问量"><span><span class="icon iconify" data-icon="ri:eye-line"></span></span><span id="busuanzi_value_site_pv"></span></span><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#9400D3" stroke-width="2" stroke-linecap="round"></circle></svg></a><a class="popup-trigger hty-icon-button icon-search" id="search" href="javascript:;" title="搜索"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:search-line"></span></span></a><script>window.addEventListener("DOMContentLoaded", () => {
  // Handle and trigger popup window
  document.querySelector(".popup-trigger").addEventListener("click", () => {
    document.querySelector(".popup").classList.add("show");
    setTimeout(() => {
      document.querySelector(".search-input").focus();
    }, 100);
  });

  // Monitor main search box
  const onPopupClose = () => {
    document.querySelector(".popup").classList.remove("show");
  };

  document.querySelector(".popup-btn-close").addEventListener("click", () => {
    onPopupClose();
  });

  window.addEventListener("keyup", event => {
    if (event.key === "Escape") {
      onPopupClose();
    }
  });
});
</script><script defer src="https://fastly.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script defer src="https://fastly.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script defer src="/js/search/algolia-search.js" type="module"></script><div class="popup search-popup"><div class="search-header"><span class="popup-btn-close close-icon hty-icon-button"><span class="icon iconify" data-icon="ri:close-line"></span></span></div><div class="search-input-container"></div><div class="algolia-results"><div id="algolia-stats"></div><div id="algolia-hits"></div><div class="algolia-pagination" id="algolia-pagination"></div></div></div><script>function initMourn() {
  const date = new Date();
  const today = (date.getMonth() + 1) + "-" + date.getDate()
  const mourn_days = ["4-4","9-18","3-3"]
  if (mourn_days.includes(today)) {
    document.documentElement.style.filter = "grayscale(1)";
  }
}
initMourn();</script></body></html>