<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#9400D3"><meta name="author" content="Joaquin Chou"><meta name="copyright" content="Joaquin Chou"><meta name="generator" content="Hexo 6.3.0"><meta name="theme" content="hexo-theme-yun"><title>SVM | 喵语小站</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...true?.options,
  });
});</script><link rel="icon" type="image/png" href="/favicon.ico"><link rel="mask-icon" href="/favicon.ico" color="#9400D3"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"www.joaquinchou.com","root":"/","title":"喵星暖暖窝","version":"1.10.11","mode":"time","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"algolia":{"appID":"L0D85KPS1S","apiKey":"715edff2e91248577c19b580f3efcb3a","indexName":"prod_NAME","hits":{"per_page":8}},"fireworks":{"colors":["0,250,154","255,69,0","33, 78, 194"]},"waline":{"config":{"enable":true,"serverURL":"https://blog-api-joaquinchou.vercel.app","comment":false,"el":"#waline","lang":"zh-CN"},"cdn":"https://fastly.jsdelivr.net/npm/@waline/client@v2/dist/waline.js","dark":"html.dark"},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><script>(function(){
  var bp = document.createElement('script');
  var curProtocol = window.location.protocol.split(':')[0];
  if (curProtocol === 'https') {
    bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
  }
  else {
    bp.src = 'http://push.zhanzhang.baidu.com/push.js';
  }
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(bp, s);
})();</script><meta name="description" content="SVM简介？ 支持向量机(Support Vector Machine)是一个二分类模型，它的基本模型是定义在特征空间上间隔最大的线性分类器，由于间隔最大使得它和感知机不同； 并且SVM还包括核技巧，这使得它成为一个非线性分类器； SVM的学习策略是间隔最大化，该策略可形式化为求解凸二次规划问题，也等价于合页损失函数最小化的问题。">
<meta property="og:type" content="article">
<meta property="og:title" content="SVM">
<meta property="og:url" content="https://www.joaquinchou.com/academic_research/machine_learning/SVM/index.html">
<meta property="og:site_name" content="喵语小站">
<meta property="og:description" content="SVM简介？ 支持向量机(Support Vector Machine)是一个二分类模型，它的基本模型是定义在特征空间上间隔最大的线性分类器，由于间隔最大使得它和感知机不同； 并且SVM还包括核技巧，这使得它成为一个非线性分类器； SVM的学习策略是间隔最大化，该策略可形式化为求解凸二次规划问题，也等价于合页损失函数最小化的问题。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-05-09T16:00:00.000Z">
<meta property="article:modified_time" content="2022-06-06T15:23:14.288Z">
<meta property="article:author" content="Joaquin Chou">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><canvas id="trianglifyContainer"></canvas><script defer src="https://fastly.jsdelivr.net/npm/trianglify@4/dist/trianglify.bundle.js"></script><script>document.addEventListener("DOMContentLoaded", () => {
  const pattern = trianglify({
    width: 800,
    height: 600,
    cellSize: 75,
    palette: ["Oranges", "RdBu", "Purples", "Blues"],
  });
  const canvasOpts = {
    applyCssScaling: false
  }
  document.body.appendChild(pattern.toCanvas(trianglifyContainer, canvasOpts));
});</script><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="Joaquin Chou"><img width="96" loading="lazy" src="https://joaquinchou-1259120139.cos.ap-guangzhou.myqcloud.com/img/img_config/JTpLkT.jpg" alt="Joaquin Chou"></a><div class="site-author-name"><a href="/about/">Joaquin Chou</a></div><a class="site-name" href="/about/site.html">喵语小站</a><sub class="site-subtitle">Hope that someone you love would also love you!</sub><div class="site-description">希望你喜欢的人她/他也爱你！</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">17</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">4</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">8</span></a></div><a class="site-state-item hty-icon-button" href="/404.html" title="还没想好呢"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/JoaquinChou" title="GitHub" target="_blank" style="color:#FF00FF"><span class="icon iconify" data-icon="ri:github-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://music.163.com/#/my/m/music/playlist?id=621567138" title="网易云音乐" target="_blank" style="color:#C20C0C"><span class="icon iconify" data-icon="ri:netease-cloud-music-line"></span></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="我的小伙伴们" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a><a class="links-item hty-icon-button" href="/albums/" title="我的相册" style="color:#DA70D6"><span class="icon iconify" data-icon="ri:gallery-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#SVM%E7%AE%80%E4%BB%8B%EF%BC%9F"><span class="toc-number">1.</span> <span class="toc-text">SVM简介？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SVM%E7%9A%84%E5%8E%9F%E7%90%86%EF%BC%9F"><span class="toc-number">2.</span> <span class="toc-text">SVM的原理？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SVM%E6%8E%A8%E5%AF%BC"><span class="toc-number">3.</span> <span class="toc-text">SVM推导</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SVM%E5%A6%82%E4%BD%95%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88%EF%BC%9F"><span class="toc-number">4.</span> <span class="toc-text">SVM如何防止过拟合？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LR%E5%92%8CSVM%E7%9A%84%E8%81%94%E7%B3%BB%E5%92%8C%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="toc-number">5.</span> <span class="toc-text">LR和SVM的联系和区别？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E6%95%B0%E9%87%8F%E4%B8%8ELR%E5%92%8CSVM%E7%9A%84%E5%85%B3%E7%B3%BB%EF%BC%9F"><span class="toc-number">6.</span> <span class="toc-text">特征数量与LR和SVM的关系？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88SVM%E5%AF%B9%E7%BC%BA%E5%A4%B1%E6%95%B0%E6%8D%AE%E6%95%8F%E6%84%9F%EF%BC%9F"><span class="toc-number">7.</span> <span class="toc-text">为什么SVM对缺失数据敏感？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%B0%86%E6%B1%82%E8%A7%A3SVM%E7%9A%84%E5%8E%9F%E5%A7%8B%E9%97%AE%E9%A2%98%E8%BD%AC%E6%8D%A2%E4%B8%BA%E5%85%B6%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">8.</span> <span class="toc-text">为什么要将求解SVM的原始问题转换为其对偶问题？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SVM%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E6%A0%B8%E5%87%BD%E6%95%B0%EF%BC%9F"><span class="toc-number">9.</span> <span class="toc-text">SVM如何选择核函数？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SVM%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9%EF%BC%9F"><span class="toc-number">10.</span> <span class="toc-text">SVM的优缺点？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SVM%E6%98%AF%E5%90%A6%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%EF%BC%9F"><span class="toc-number">11.</span> <span class="toc-text">SVM是否可以使用随机梯度下降算法？</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#9400D3;"><link itemprop="mainEntityOfPage" href="https://www.joaquinchou.com/academic_research/machine_learning/SVM/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="Joaquin Chou"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="喵语小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">SVM</h1><div class="post-meta"><div class="post-time" style="display:block"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="创建时间：2022-05-10 00:00:00" itemprop="dateCreated datePublished" datetime="2022-05-10T00:00:00+08:00">2022-05-10</time></div><span class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><span class="icon iconify" data-icon="ri:file-word-line"></span></span> <span title="本文字数">1.9k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><span class="icon iconify" data-icon="ri:timer-line"></span></span> <span title="阅读时长">7m</span></span></span><span class="post-busuanzi"><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读次数"><span class="icon iconify" data-icon="ri:eye-line"></span> <span id="busuanzi_value_page_pv"></span></span></span><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/%E5%96%B5%E8%AF%AD%E7%AC%94%E8%AE%B0/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">喵语笔记</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="--text-color:#3776ab"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">机器学习</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><h2 id="SVM简介？"><a href="#SVM简介？" class="headerlink" title="SVM简介？"></a>SVM简介？</h2><ul>
<li>支持向量机(Support Vector Machine)是一个二分类模型，它的<b>基本模型</b>是定义在特征空间上<b>间隔最大</b>的线性分类器，由于间隔最大使得它和感知机不同；</li>
<li>并且SVM还包括<b>核技巧</b>，这使得它成为一个非线性分类器；</li>
<li>SVM的学习策略是<b>间隔最大化</b>，该策略可形式化为求解凸二次规划问题，也等价于合页损失函数最小化的问题。</li>
</ul>
<span id="more"></span>

<h2 id="SVM的原理？"><a href="#SVM的原理？" class="headerlink" title="SVM的原理？"></a>SVM的原理？</h2><ul>
<li>SVM的<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/31886934">基本想法</a>是最大化离超平面最近的点到该平面的距离。</li>
<li>对于线性可分的数据集来说，可分离的超平面有无数个，但是几何间隔最大的超平面只有一个；</li>
<li>对于线性可分的数据集，可以把问题转化为凸优化问题，再使用<b>拉格朗日乘子法简化</b>求解；</li>
<li>对于线性不可分的数据集，可以使用核函数将<b>样本映射到高维空间</b>中，使它变成线性可分的数据再求解。</li>
</ul>
<h2 id="SVM推导"><a href="#SVM推导" class="headerlink" title="SVM推导"></a>SVM推导</h2><p>强推：<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/tj_O8H3S_kdag30jivBy6g">svm原理从头到尾详细推导</a></p>
<p>大致过程：</p>
<ul>
<li>SVM要求解的问题是最大化离超平面最近点到超平面的距离。即优化如下问题：<br>$$\underset{\omega,b}{max}(\underset{x_i}{min}\frac{y_i(\omega ^Tx_i+b)}{||\omega||})$$</li>
<li>为了简化计算，假设最近点离超平面的距离为1，此时其他点的函数距离$y_i(\omega^T x_i+b)&gt;1$。问题转化为在$y_i(\omega^T x_i+b)≥1$的不等式约束下关于$\omega$的最小化问题：<br>$$\underset{\omega,b}{max}\frac{1}{||\omega||}=&gt;\underset{\omega,b}{min}\frac{1}{2}||\omega||^2 \quad\quad\quad s.t.\quad y_i(\omega^T x_i+b)≥1$$</li>
<li>为了方便求解上式，需要把不等式约束问题转化为无约束的极小极大值问题，并且该优化结果满足kkt条件(拉格朗日乘子法的泛化)，所以可以进一步把不等式的约束问题转化为它的对偶问题(极小极大值问题)。再利用kkt条件求导可以求解最终参数结果。   </li>
<li>为了防止由于标签标记错误(离群点)对模型性能产生影响，此时会对离群点引入松弛因子$\beta_i$，加入后的损失函数为<br>$$\underset{\omega,b,\beta}{min}(\frac{1}{2}||\omega||^2+C\overset{m}{\underset{i=1}{\sum}}\beta_i)\quad\quad\quad s.t.\quad y_i(\omega^T x_i+b)≥1-\beta_i\beta_i≥0$$<br>$\beta$表示超平面两侧离群点之间的距离，$C$表示对离群点的重视程度。当$\beta_i$固定时，$C=0$表示可以忽略这些离群点，$C=+\infty$表示离群点非常重要，此时会导致模型线性不可分(可以借助核函数映射到线性空间)。增大惩罚因子$C$，模型的泛化能力会变弱，$C=+\infty$时，模型退化为<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/81890745">线性可分的SVM(硬间隔)</a>，减小惩罚因子，模型的泛化能力变好。</li>
</ul>
<h2 id="SVM如何防止过拟合？"><a href="#SVM如何防止过拟合？" class="headerlink" title="SVM如何防止过拟合？"></a>SVM如何防止过拟合？</h2><p>对离群点引入松弛因子$\beta_i$，SVM希望通过引入松弛因子$\beta_i$和惩罚因子$C$来容忍离群点的存在，$C$越大，表示越重视离群点，$C$越小，表示可以忽略该离群点的存在。</p>
<h2 id="LR和SVM的联系和区别？"><a href="#LR和SVM的联系和区别？" class="headerlink" title="LR和SVM的联系和区别？"></a>LR和SVM的联系和区别？</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/julyedu_7/article/details/121836689">参考</a></p>
<ul>
<li><p>联系：</p>
<ul>
<li>一般都用来处理二分类问题；</li>
<li>都是线性分类器，本质上都是求一个最佳的分类超平面；</li>
<li>都是监督学习算法；</li>
<li>都是判别模型，判别模型不关注数据的生成，只关注数据之间的差别。</li>
</ul>
</li>
<li><p>区别：</p>
<ul>
<li>LR是参数模型(假设输出服从二项分布)，SVM是非参数模型；</li>
<li>解决非线性问题时，SVM采用核函数，LR一般不采用；(因为LR考虑所有的样本，SVM只依赖于决策面附近的支持向量，计算量不一样)</li>
<li>SVM的损失函数自带正则化，而LR需要在损失函数之外另外加入(如下)；</li>
<li>从损失函数上看，LR的损失函数是交叉熵，是基于概率分类的，如下：<br>$$J(\theta)=-\frac{1}{m}[\overset{m}{\underset{i=1}{\sum}}y^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]$$<br>SVM的损失函数是合页损失函数，是基于几何间隔最大化的原理，是基于几何距离进行分类的，如下：<br>$$L(\omega,b,\alpha)=\frac{1}{2}||w||^2-\overset{m}{\underset{i=1}{\sum}}\alpha_i(y_i(\omega^Tx_i+b)-1)$$</li>
</ul>
</li>
</ul>
<p>[注]</p>
<ul>
<li>常见的判别模型有LR，KNN，SVM，常见的生成模型有朴素贝叶斯，隐马尔可夫模型；</li>
<li>参数模型是指假设数据总体服从某个分布，这个分布可以由具体数量的参数确定<font color='red'>(参数确定，模型就确定了)</font>；非参数模型是指数据分布未知，无法得到分布的相关参数，只有在给定一些样本的条件下，通过非参数估计的方法推断。</li>
</ul>
<h2 id="特征数量与LR和SVM的关系？"><a href="#特征数量与LR和SVM的关系？" class="headerlink" title="特征数量与LR和SVM的关系？"></a>特征数量与LR和SVM的关系？</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/46943307">参考</a></p>
<ul>
<li>如果特征数量很大，和样本数量差不多，选用LR或者线性核的SVM；</li>
<li>如果特征数量比较少，样本数量不多不少，选择RBF核(高斯核)的SVM；</li>
<li>如果特征数量比较少，样本数量很多，需要手动添加特征再选用线性核的SVM。</li>
</ul>
<h2 id="为什么SVM对缺失数据敏感？"><a href="#为什么SVM对缺失数据敏感？" class="headerlink" title="为什么SVM对缺失数据敏感？"></a>为什么SVM对缺失数据敏感？</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/81890745">参考</a></p>
<p>一般的缺失数据指数据的特征不完整，<b>SVM没有处理缺失值的策略</b>。而且SVM希望样本在特征空间中是线性可分的，所以特征空间的好坏会影响SVM的性能，缺失某些数据的特征会影响训练的结果。</p>
<h2 id="为什么要将求解SVM的原始问题转换为其对偶问题？"><a href="#为什么要将求解SVM的原始问题转换为其对偶问题？" class="headerlink" title="为什么要将求解SVM的原始问题转换为其对偶问题？"></a>为什么要将求解SVM的原始问题转换为其对偶问题？</h2><p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/tj_O8H3S_kdag30jivBy6g">参考</a></p>
<ul>
<li>因为原问题是个带有不等式约束的问题，为了方便求解，可以把目标函数和约束全部融入拉格朗日函数，再求解其对偶问题，把原问题转化为无约束的极小极大值问题，利用kkt条件求解；</li>
<li>转化为对偶问题可以自然地引入核函数，进而推广到非线性问题。</li>
</ul>
<h2 id="SVM如何选择核函数？"><a href="#SVM如何选择核函数？" class="headerlink" title="SVM如何选择核函数？"></a>SVM如何选择核函数？</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/81890745">参考</a></p>
<ul>
<li>当特征维度较大，样本数很小时(文本分类)，使用线性核；</li>
<li>当特征维度较小，样本数不大不小时，使用高斯核(RBF核)；</li>
<li>当特征维度较小，样本数较多时，此时SVM的性能不如神经网络。</li>
</ul>
<h2 id="SVM的优缺点？"><a href="#SVM的优缺点？" class="headerlink" title="SVM的优缺点？"></a>SVM的优缺点？</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/81890745">参考</a></p>
<ul>
<li>优点<ul>
<li>SVM是个凸优化问题，所以求得的解是一个全部最优而不是局部最优解；</li>
<li>不仅适用于线性问题，使用核技巧后还能处理非线性问题；</li>
<li>对于高维样本空间的数据也能使用，因为数据集的复杂度只取决于支持向量而不是数据集的维度，可以避免“维度灾难”。</li>
</ul>
</li>
<li>缺点<ul>
<li>二次规划问题求解设计$m$阶矩阵的计算($m$为样本数)，所以SVM不适合超大数据集(SMO算法可缓解这个问题)；</li>
<li>样本数量比较多，效果不如神经网络。</li>
</ul>
</li>
</ul>
<h2 id="SVM是否可以使用随机梯度下降算法？"><a href="#SVM是否可以使用随机梯度下降算法？" class="headerlink" title="SVM是否可以使用随机梯度下降算法？"></a>SVM是否可以使用随机梯度下降算法？</h2><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/265751466/answer/1215966308">参考</a></p>
<p>SVM的求解本质上是个带约束的二次规划问题，可以通过拉格朗日乘子法或$Hinge Loss$转化为无约束的优化问题。$Hinge Loss$是个凸函数，可以使用随机梯度下降优化，只是使用拉格朗日乘子法对SVM优化有个好处，如果SVM有使用到核技巧，使用SMO算法直接求解比使用随机梯度下降更高效。</p>
</div></section><div id="reward-container"><span class="hty-icon-button button-glow" id="reward-button" title="打赏" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === &quot;none&quot;) ? &quot;block&quot; : &quot;none&quot;;"><span class="icon iconify" data-icon="ri:hand-coin-line"></span></span><div id="reward-comment">来都来了，不点一下麽？(●ˇ∀ˇ●)</div><div id="qr" style="display:none;"><div style="display:inline-block"><a target="_blank" rel="noopener" href="https://joaquinchou-1259120139.cos.ap-guangzhou.myqcloud.com/img/img_config/alipay.jpg"><img loading="lazy" src="https://joaquinchou-1259120139.cos.ap-guangzhou.myqcloud.com/img/img_config/alipay.jpg" alt="支付宝" title="支付宝"></a><div><span style="color:#00A3EE"><span class="icon iconify" data-icon="ri:alipay-line"></span></span></div></div><div style="display:inline-block"><a target="_blank" rel="noopener" href="https://joaquinchou-1259120139.cos.ap-guangzhou.myqcloud.com/img/img_config/QQ.png"><img loading="lazy" src="https://joaquinchou-1259120139.cos.ap-guangzhou.myqcloud.com/img/img_config/QQ.png" alt="QQ 支付" title="QQ 支付"></a><div><span style="color:#12B7F5"><span class="icon iconify" data-icon="ri:qq-line"></span></span></div></div><div style="display:inline-block"><a target="_blank" rel="noopener" href="https://joaquinchou-1259120139.cos.ap-guangzhou.myqcloud.com/img/img_config/wechat.png"><img loading="lazy" src="https://joaquinchou-1259120139.cos.ap-guangzhou.myqcloud.com/img/img_config/wechat.png" alt="微信支付" title="微信支付"></a><div><span style="color:#2DC100"><span class="icon iconify" data-icon="ri:wechat-pay-line"></span></span></div></div></div></div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>Joaquin Chou</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="https://www.joaquinchou.com/academic_research/machine_learning/SVM/" title="SVM">https://www.joaquinchou.com/academic_research/machine_learning/SVM/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> 许可协议。</li></ul><script>document.addEventListener('copy', function (event) {
  const clipboardData = event.clipboardData || window.clipboardData;
  if (!clipboardData) { return; }
  const text = window.getSelection().toString();
  if (text) {
    event.preventDefault();
    clipboardData.setData('text/plain', text + '\n\n本文作者：Joaquin Chou\n本文链接：https://www.joaquinchou.com/academic_research/machine_learning/SVM/\n版权声明：本博客所有文章除特别声明外，均默认采用 CC BY-NC-SA 4.0 许可协议。');
  }
});</script></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/personal_learning/leetcode/leetcode_JianZhi/" rel="prev" title="Leetcode_and_JianZhi日常复习"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">Leetcode_and_JianZhi日常复习</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/academic_research/machine_learning/KNN/" rel="next" title="k近邻算法"><span class="post-nav-text">k近邻算法</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"><div class="comment-tooltip text-center"><span>可以点击下方进行评论哟！</span><br></div><div id="waline"></div><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@waline/client@v2/dist/waline.css"><script>window.CONFIG.waline.config.path = "/academic_research/machine_learning/SVM/"</script><div class="js-Pjax"><script src="/js/comments/waline.js" type="module" defer></script></div></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2023 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> Joaquin Chou</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v6.3.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.11</span></div><div class="live-time"><span>本博客已萌萌哒地运行</span><span id="display_live_time"></span><span class="moe-text">(●'◡'●)</span><script>function blog_live_time() {
  setTimeout(blog_live_time, 1000);
  const start = new Date('2020-11-12T00:00:00');
  const now = new Date();
  const timeDiff = (now.getTime() - start.getTime());
  const msPerMinute = 60 * 1000;
  const msPerHour = 60 * msPerMinute;
  const msPerDay = 24 * msPerHour;
  const passDay = Math.floor(timeDiff / msPerDay);
  const passHour = Math.floor((timeDiff % msPerDay) / 60 / 60 / 1000);
  const passMinute = Math.floor((timeDiff % msPerHour) / 60 / 1000);
  const passSecond = Math.floor((timeDiff % msPerMinute) / 1000);
  display_live_time.innerHTML = ` ${passDay} 天 ${passHour} 小时 ${passMinute} 分 ${passSecond} 秒`;
}
blog_live_time();
</script></div><div id="busuanzi"><span id="busuanzi_container_site_uv" title="总访客量"><span><span class="icon iconify" data-icon="ri:user-line"></span></span><span id="busuanzi_value_site_uv"></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv" title="总访问量"><span><span class="icon iconify" data-icon="ri:eye-line"></span></span><span id="busuanzi_value_site_pv"></span></span><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#9400D3" stroke-width="2" stroke-linecap="round"></circle></svg></a><a class="popup-trigger hty-icon-button icon-search" id="search" href="javascript:;" title="搜索"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:search-line"></span></span></a><script>window.addEventListener("DOMContentLoaded", () => {
  // Handle and trigger popup window
  document.querySelector(".popup-trigger").addEventListener("click", () => {
    document.querySelector(".popup").classList.add("show");
    setTimeout(() => {
      document.querySelector(".search-input").focus();
    }, 100);
  });

  // Monitor main search box
  const onPopupClose = () => {
    document.querySelector(".popup").classList.remove("show");
  };

  document.querySelector(".popup-btn-close").addEventListener("click", () => {
    onPopupClose();
  });

  window.addEventListener("keyup", event => {
    if (event.key === "Escape") {
      onPopupClose();
    }
  });
});
</script><script defer src="https://fastly.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script defer src="https://fastly.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script defer src="/js/search/algolia-search.js" type="module"></script><div class="popup search-popup"><div class="search-header"><span class="popup-btn-close close-icon hty-icon-button"><span class="icon iconify" data-icon="ri:close-line"></span></span></div><div class="search-input-container"></div><div class="algolia-results"><div id="algolia-stats"></div><div id="algolia-hits"></div><div class="algolia-pagination" id="algolia-pagination"></div></div></div><script>function initMourn() {
  const date = new Date();
  const today = (date.getMonth() + 1) + "-" + date.getDate()
  const mourn_days = ["4-4","9-18","3-3"]
  if (mourn_days.includes(today)) {
    document.documentElement.style.filter = "grayscale(1)";
  }
}
initMourn();</script></body></html>